{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "DialoGPT-large loaded on device: cuda\n",
      "generate_response function defined.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load the GPU-enabled DialoGPT-large model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\").to(device)\n",
    "print(\"DialoGPT-large loaded on device:\", device)\n",
    "\n",
    "def generate_response(prompt):\n",
    "    \"\"\"\n",
    "    Generate a response using DialoGPT-large on GPU with sampling parameters\n",
    "    tuned for coherent output, and strip the prompt if it is repeated.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors=\"pt\").to(device)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long().to(device)\n",
    "    output_ids = model_llm.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,       # Lower temperature for more coherent, less random output\n",
    "        top_p=0.9,             # Top-p sampling to focus on high-probability tokens\n",
    "        repetition_penalty=1.1, # Moderate repetition penalty to avoid loops\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    if response.lower().startswith(prompt.lower()):\n",
    "        response = response[len(prompt):].strip()\n",
    "    return response\n",
    "\n",
    "print(\"generate_response function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available voices:\n",
      "0: Microsoft David Desktop - English (United States) (HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0)\n",
      "1: Microsoft Zira Desktop - English (United States) (HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0)\n",
      "Model 1 voice ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0\n",
      "Model 2 voice ID: HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "\n",
    "# Initialize two TTS engines.\n",
    "engine1 = pyttsx3.init()\n",
    "engine2 = pyttsx3.init()\n",
    "\n",
    "# List available voices.\n",
    "voices = engine1.getProperty('voices')\n",
    "print(\"Available voices:\")\n",
    "for idx, voice in enumerate(voices):\n",
    "    print(f\"{idx}: {voice.name} ({voice.id})\")\n",
    "\n",
    "# Set voices. If you have at least two voices, use them; otherwise, manually override Model 2's voice.\n",
    "if len(voices) >= 2:\n",
    "    engine1.setProperty('voice', voices[0].id)\n",
    "    engine2.setProperty('voice', voices[1].id)\n",
    "else:\n",
    "    engine1.setProperty('voice', voices[0].id)\n",
    "    # Example manual override (adjust as needed):\n",
    "    # engine2.setProperty('voice', \"HKEY_LOCAL_MACHINE\\\\SOFTWARE\\\\Microsoft\\\\Speech\\\\Voices\\\\Tokens\\\\TTS_MS_EN-US_ZIRA_11.0\")\n",
    "    engine2.setProperty('voice', voices[0].id)\n",
    "\n",
    "print(\"Model 1 voice ID:\", engine1.getProperty('voice'))\n",
    "print(\"Model 2 voice ID:\", engine2.getProperty('voice'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speak_text and capture_tts functions defined.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import tempfile\n",
    "import uuid\n",
    "from playsound import playsound\n",
    "\n",
    "def speak_text(engine, text):\n",
    "    \"\"\"Speak the given text using the provided pyttsx3 engine.\"\"\"\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def capture_tts(speaker_engine, text):\n",
    "    \"\"\"\n",
    "    Generate TTS output using speaker_engine by saving to a unique file,\n",
    "    then transcribe that file using Whisper (without playing it).\n",
    "    Returns the transcription.\n",
    "    \"\"\"\n",
    "    unique_filename = f\"tts_output_{uuid.uuid4().hex}.wav\"\n",
    "    speaker_engine.save_to_file(text, unique_filename)\n",
    "    speaker_engine.runAndWait()\n",
    "    time.sleep(1)  # Allow time for the file to be released.\n",
    "    transcription = transcribe_audio(unique_filename)\n",
    "    try:\n",
    "        os.remove(unique_filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: could not remove temporary file {unique_filename}: {e}\")\n",
    "    return transcription\n",
    "\n",
    "print(\"speak_text and capture_tts functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_audio function defined.\n",
      "Loading Whisper model...\n",
      "Whisper model loaded.\n",
      "transcribe_audio function defined.\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import pyaudio\n",
    "import whisper\n",
    "\n",
    "def record_audio(duration=10, rate=16000, chunk_size=1024):\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16, channels=1, rate=rate, input=True, frames_per_buffer=chunk_size)\n",
    "    frames = []\n",
    "    print(\"Recording...\")\n",
    "    for i in range(0, int(rate / chunk_size * duration)):\n",
    "        data = stream.read(chunk_size)\n",
    "        frames.append(data)\n",
    "    print(\"Finished recording.\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
    "    wf = wave.open(temp_file.name, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "    wf.setframerate(rate)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    return temp_file.name\n",
    "\n",
    "print(\"record_audio function defined.\")\n",
    "\n",
    "print(\"Loading Whisper model...\")\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "print(\"Whisper model loaded.\")\n",
    "\n",
    "def transcribe_audio(audio_filename):\n",
    "    result = whisper_model.transcribe(audio_filename, language=\"en\")\n",
    "    return result[\"text\"]\n",
    "\n",
    "print(\"transcribe_audio function defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partner's dynamic recording functions defined.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_rms(audio_buffer):\n",
    "    audio_buffer = audio_buffer.astype(np.float32)\n",
    "    audio_buffer[np.isnan(audio_buffer) | np.isinf(audio_buffer)] = 0\n",
    "    return np.sqrt(np.mean(np.square(audio_buffer)))\n",
    "\n",
    "def save_audio(filename, audio_data, rate=16000):\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(rate)\n",
    "        wf.writeframes(audio_data)\n",
    "    return os.path.abspath(filename)\n",
    "\n",
    "def partner_record_audio(duration, rate=16000, chunk_size=1600):\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=rate,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=chunk_size)\n",
    "    frames = []\n",
    "    for _ in range(0, int(rate / chunk_size * duration)):\n",
    "        data = stream.read(chunk_size)\n",
    "        frames.append(data)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    audio_data = b''.join(frames)\n",
    "    return audio_data\n",
    "\n",
    "def monitor_and_record(scalar=0.5, duration_for_average=8, rate=16000, chunk_size=16000):\n",
    "    print(\"Monitoring loudness...\")\n",
    "    initial_audio = partner_record_audio(duration_for_average, rate, chunk_size)\n",
    "    average_loudness = calculate_rms(np.frombuffer(initial_audio, dtype=np.int16).astype(np.float32))\n",
    "    print(f\"Average loudness (RMS) in the room: {average_loudness}\")\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=rate,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=chunk_size)\n",
    "    frames = []\n",
    "    is_recording = False\n",
    "    upThresh = (1.0 + scalar) * average_loudness\n",
    "    lowThresh = (1.0 - scalar) * average_loudness\n",
    "    print(\"upThresh:\", upThresh)\n",
    "    print(\"lowThresh:\", lowThresh)\n",
    "    print(\"Listening for loudness change...\")\n",
    "    try:\n",
    "        while True:\n",
    "            data = stream.read(chunk_size)\n",
    "            audio_data = np.frombuffer(data, dtype=np.int16).copy()\n",
    "            audio_data[np.isnan(audio_data) | np.isinf(audio_data)] = 32767\n",
    "            current_loudness = calculate_rms(audio_data)\n",
    "            print(\"Current loudness:\", current_loudness)\n",
    "            if not np.isnan(current_loudness):\n",
    "                if current_loudness > upThresh and not is_recording:\n",
    "                    print(\"Loudness exceeded threshold. Starting recording...\")\n",
    "                    is_recording = True\n",
    "                    frames = [data]\n",
    "                elif current_loudness < lowThresh and is_recording:\n",
    "                    print(\"Loudness dropped below threshold. Stopping recording...\")\n",
    "                    is_recording = False\n",
    "                    break\n",
    "                elif is_recording:\n",
    "                    frames.append(data)\n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "    if frames:\n",
    "        print(\"Saving recorded audio...\")\n",
    "        audio_data = b''.join(frames)\n",
    "        saved_file = save_audio(\"recorded_audio.wav\", audio_data, rate)\n",
    "        print(\"Saved recorded audio to:\", saved_file)\n",
    "        return saved_file\n",
    "\n",
    "print(\"Partner's dynamic recording functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please speak your initial prompt. Wait for the system to detect and record your speech...\n",
      "Monitoring loudness...\n",
      "Average loudness (RMS) in the room: 52.58523941040039\n",
      "upThresh: 78.87785911560059\n",
      "lowThresh: 26.292619705200195\n",
      "Listening for loudness change...\n",
      "Current loudness: 88.90449\n",
      "Loudness exceeded threshold. Starting recording...\n",
      "Current loudness: 97.04795\n",
      "Current loudness: 417.6359\n",
      "Current loudness: 1627.999\n",
      "Current loudness: 679.0152\n",
      "Current loudness: 145.56215\n",
      "Current loudness: 38.05188\n",
      "Current loudness: 37.442146\n",
      "Current loudness: 36.069153\n",
      "Current loudness: 502.60876\n",
      "Current loudness: 978.72437\n",
      "Current loudness: 624.9519\n",
      "Current loudness: 581.0121\n",
      "Current loudness: 597.4029\n",
      "Current loudness: 413.85434\n",
      "Current loudness: 127.341835\n",
      "Current loudness: 41.100338\n",
      "Current loudness: 55.496235\n",
      "Current loudness: 8.75051\n",
      "Loudness dropped below threshold. Stopping recording...\n",
      "Saving recorded audio...\n",
      "Saved recorded audio to: d:\\usuhackathon2025\\Datathon\\CelestialChoreography\\CelestialChoreography\\Data\\recorded_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Lib\\site-packages\\whisper\\model.py:124: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  a = scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribed initial prompt:  Yeah, we're all on 312. Use a pip on, use pip to uninstall whisper again if you're having trouble.\n",
      "Model 1 says: , I'm pretty sure that's how it works.'Tis the best way of doing things for me anyway! 306 Warlock with high charisma and low luck... haha :D psn is same as username? oops lolwutupooguymfwocuteymee 8pypi michiganen 1st werido1s\n",
      "\n",
      "--- Conversation Round 1 ---\n",
      "Model 2 hears:  I'm pretty sure that's how it works Tis the best way of doing things for me anyway. 306 Warlock with high charisma and low luck, ha ha, laughing face, PSN is same as username? Oops low you to put Gimphwakity me 8PYP I Michigan in first where I don't want S.\n",
      "Model 2 says: .? edit : spelling mistakes are bad, btw! sry'thx m8s lt 3 u 2o7m4mee\n",
      "Model 1 hears:  Edit, spelling mistakes are bad, BTW. SRED-THX-MH-S-L-T-3-U-207-M for me.\n",
      "Model 1 says: some of us don't know the difference. s EDIT : Thanks! I didn t notice that it was a typo and not just my phone keyboard haha...? HAHaha sorry about your dyslexia lt 3 you guys r adorablee here, lolol pats on head u changetip 100 bits to OP\n",
      "\n",
      "--- Conversation Round 2 ---\n",
      "Model 2 hears:  Some of us don't know the difference. Yes edit, thanks. I didn't T notice that it was a typo and not just my phone keyboard ha ha. Ha ha ha sorry about your dyslexia LT3 you guys are adorablee here. LOLOLPATS on head you change a tip 100 bits to OP.\n",
      "Model 2 says: ! 1. u changetip 50 bitsss thank YOU for giving me gold :D DDDd lt 3 Thank You So much love, We Are\n",
      "Model 1 hears:  1. You change a tip 50 bits thank you for giving me gold, laughing face, ddddlt3 thank you so much love, we are.\n",
      "Model 1 says: 're all on this together.u changetip 100 bit u ChangeTipBot megaflipcoin verify Next time, it's not that funny :P r bitcoin is full of these people! lt 3 Thanks and have fun with your new money to mine in the future... bye now btw o7o0a1s\n",
      "\n",
      "--- Conversation Round 3 ---\n",
      "Model 2 hears:  Reall on this together.You change a tip 100 bit you change tip bot mega flip coin verify next time. It's not that funny. Tung sticking out. Our bitcoin is full of these people. LT3 thanks and have fun with your new money to mine in the future. By now BTW 7001s.\n",
      "Model 2 says: 1 btc u changetip 13563 satoshi Btw, how many bitcoins are left? Are they really worth anything anymore or no longer valuable at all.\n"
     ]
    }
   ],
   "source": [
    "# ----- Main Conversation Loop -----\n",
    "\n",
    "# Step 1: Capture the initial spoken prompt dynamically.\n",
    "print(\"Please speak your initial prompt. Wait for the system to detect and record your speech...\")\n",
    "initial_audio_file = monitor_and_record(scalar=0.5, duration_for_average=8, rate=16000, chunk_size=16000)\n",
    "initial_prompt = transcribe_audio(initial_audio_file)\n",
    "os.remove(initial_audio_file)\n",
    "print(\"Transcribed initial prompt:\", initial_prompt)\n",
    "\n",
    "# Step 2: Model 1 generates its first reply from the transcribed initial prompt.\n",
    "response_text_1 = generate_response(initial_prompt)\n",
    "print(\"Model 1 says:\", response_text_1)\n",
    "speak_text(engine1, response_text_1)\n",
    "time.sleep(2)\n",
    "\n",
    "# Use Model 1's first reply as the conversation context.\n",
    "current_context = response_text_1\n",
    "\n",
    "# Step 3: Conversation Loop: Exchange responses using file-captured TTS.\n",
    "num_rounds = 10  # Adjust as desired.\n",
    "for i in range(num_rounds):\n",
    "    print(f\"\\n--- Conversation Round {i+1} ---\")\n",
    "    \n",
    "    # Model 2 captures Model 1's current reply via file capture.\n",
    "    transcription_model1 = capture_tts(engine1, current_context)\n",
    "    print(\"Model 2 hears:\", transcription_model1)\n",
    "    \n",
    "    # Model 2 generates its reply based on the captured text.\n",
    "    response_text_2 = generate_response(transcription_model1)\n",
    "    print(\"Model 2 says:\", response_text_2)\n",
    "    speak_text(engine2, response_text_2)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Model 1 captures Model 2's reply via file capture.\n",
    "    transcription_model2 = capture_tts(engine2, response_text_2)\n",
    "    print(\"Model 1 hears:\", transcription_model2)\n",
    "    \n",
    "    # Model 1 generates its new reply.\n",
    "    response_text_1 = generate_response(transcription_model2)\n",
    "    print(\"Model 1 says:\", response_text_1)\n",
    "    speak_text(engine1, response_text_1)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Update conversation context for the next round.\n",
    "    current_context = response_text_1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
